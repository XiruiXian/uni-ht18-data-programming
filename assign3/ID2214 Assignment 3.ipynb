{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ID2214 Assignment 3.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["827zKx7Z4JP8"],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"metadata":{"id":"O3TdCeop4JPm","colab_type":"text"},"cell_type":"markdown","source":["# ID2214 Assignment 3 Group no. 5\n","### Project members: \n","[Ceren Dikmen, cerend@kth.se] [Jakob Heyder, heyder@kth.se] [Lutfi Altin, lutfia@kth.se] [Muhammad Fasih Ullah, mufu@kth.se]\n","\n","### Declaration\n","By submitting this solution, it is hereby declared that all individuals listed above have contributed to the solution, either with code that appear in the final solution below, or with code that has been evaluated and compared to the final solution, but for some reason has been excluded. It is also declared that all project members fully understand all parts of the final solution and can explain it upon request.\n","\n","It is furthermore declared that the code below is a contribution by the project members only, and specifically that no part of the solution has been copied from any other source (except for lecture slides at the course ID2214) and no part of the solution has been provided by someone not listed as project member above.\n","\n","It is furthermore declared that it has been understood that no other library/package than the Python 3 standard library, NumPy, pandas and time may be used in the solution for this assignment.\n","\n","### Instructions\n","All assignments starting with number 1 below are mandatory. Satisfactory solutions\n","will give 1 point (in total). If they in addition are good (all parts work more or less \n","as they should), completed on time (submitted before the deadline in Canvas) and according\n","to the instructions, together with satisfactory solutions of assignments starting with \n","number 2 below, then the assignment will receive 2 points (in total).\n","\n","It is highly recommended that you do not develop the code directly within the notebook\n","but that you copy the comments and test cases to your regular development environment\n","and only when everything works as expected, that you paste your functions into this\n","notebook, do a final testing (all cells should succeed) and submit the whole notebook \n","(a single file) in Canvas (do not forget to fill in your group number and names above).\n"]},{"metadata":{"id":"quwYUM714JPs","colab_type":"text"},"cell_type":"markdown","source":["## Load NumPy, pandas and time"]},{"metadata":{"id":"1xuLy7Jl4JPv","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import time\n","import pprint\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"827zKx7Z4JP8","colab_type":"text"},"cell_type":"markdown","source":["## Reused functions from Assignment 1"]},{"metadata":{"id":"eyVMHMbK4JP-","colab_type":"code","colab":{}},"cell_type":"code","source":["# Copy and paste functions from Assignment 1 here that you need for this assignment\n","\n","def create_bins(df, nobins = 10, bintype = \"equal-width\"):\n","    copydf = df.copy()\n","    binning = {}\n","    \n","    cols = copydf.loc[:, ~df.columns.isin(['ID', 'CLASS'])]\n","    numbercols = cols.select_dtypes(include=[np.int_, np.float_])\n","    \n","    for col in numbercols:\n","        cutfunc = pd.cut if bintype == \"equal-width\" else pd.qcut\n","        copydf[col], bins = cutfunc(copydf[col], nobins, labels=False, retbins=True, duplicates=\"drop\")\n","        # hint 4-5\n","        copydf[col] = copydf[col].astype('category', categories=list(range(nobins)))\n","        \n","        # hint 6\n","        bins[0] = -np.inf\n","        bins[-1] = np.inf\n","        binning[col] = bins\n","        \n","    return copydf, binning\n","\n","def apply_bins(df, binning):\n","    copydf = df.copy()\n","    \n","    for column in binning:\n","        copydf[column] = pd.cut(copydf[column], labels=False, bins=binning[column])\n","        \n","        # hint 3-4\n","        nobins = len(binning[column] - 1) # n bins will generate n+1 values\n","        copydf[column] = copydf[column].astype('category', categories=list(range(nobins)))\n","    \n","    return copydf\n","\n","def create_normalization(df, normalizationtype = \"minmax\"):\n","    copydf = df.copy()\n","    normalization = {}\n","    \n","    cols = copydf.loc[:, ~df.columns.isin(['ID', 'CLASS'])] # Removing ID and CLASS columns\n","    cols = cols.select_dtypes(include=[np.int_, np.float_]) # Selecting only int and float type columns from the remaining\n","    \n","    if normalizationtype == 'minmax':\n","        for column in cols:\n","            min = copydf[column].min()\n","            max = copydf[column].max()\n","            copydf[column] = [(x-min)/(max-min) for x in copydf[column]]\n","            normalization[column] = (\"minmax\", min, max)\n","            \n","    elif normalizationtype == 'zscore':\n","        for column in cols:\n","            mean = copydf[column].mean()\n","            std = copydf[column].std()\n","            copydf[column] = copydf[column].apply(lambda x: (x-mean)/std)\n","            normalization[column] = (\"zscore\", mean, std)\n","            \n","    return copydf, normalization\n","\n","def apply_normalization(df, normalization):\n","    copydf = df.copy()\n","    \n","    for column in normalization:\n","        type = normalization[column][0]\n","        \n","        if type == 'minmax':\n","            min = normalization[column][1]\n","            max = normalization[column][2]\n","            copydf[column] = np.clip([(x-min)/(max-min) for x in copydf[column]], 0, 1) # Clipping outliers to 0-1\n","            \n","        elif type == 'zscore':\n","            mean = normalization[column][1]\n","            std = normalization[column][2]\n","            copydf[column] = copydf[column].apply(lambda x: (x-mean)/std)\n","   \n","    return copydf\n","\n","\n","def create_imputation(df):\n","    copydf = df.copy()\n","    imputation = {}\n","    \n","    cols = copydf.loc[:, ~df.columns.isin(['ID', 'CLASS'])] # Dropping ID and CLASS columns\n","    numbercols = cols.select_dtypes(include=[np.int_, np.float_]) # Choosing int and float columns\n","    othercols = cols.select_dtypes(exclude=[np.int_, np.float_]) # Choosing all other columns \n","    \n","    for column in numbercols:\n","        copydf[column].fillna(copydf[column].mean(),inplace=True) # For numeric columns replacing them with mean\n","        imputation[column] = copydf[column].mean()\n","        \n","        if copydf[column].isna().any(): # Check if whole column was NaN\n","            copydf[column].fillna(0, inplace=True) #Replace with 0\n","            imputation[column] = 0\n","        \n","    for column in othercols:\n","        # Mode returns an array. In order to get the correct mode we have to use iloc[0]\n","        copydf[column].fillna(copydf[column].mode().iloc[0],inplace=True)\n","        imputation[column] = copydf[column].mode().iloc[0]\n","        \n","        # Check if column still has some NA \n","        # if it does, change it to category[to match the output] and replace it with the first category\n","        if copydf[column].isna().any():\n","          \n","            copydf[column] = copydf[column].astype('category')\n","            \n","            # [Old code, doesn't work] fill = \"\" if copydf[column].dtype == 'object' else df.cat.categories[0]\n","            \n","            fill = \"\" if copydf[column].dtype == 'object' else copydf[column].astype('category').cat.categories[0]\n","            copydf[column].fillna(fill, inplace=True)\n","            imputation[column] = fill\n","            \n","            # print(\"CopyDF: \", column, copydf[column].dtype, copydf[column].astype('category').cat.categories[0])\n","            \n","    return copydf, imputation\n","\n","def apply_imputation(df, imputation):\n","    copydf = df.copy()\n","    \n","    for column in imputation:\n","        copydf[column].fillna(imputation[column], inplace=True)\n","    \n","    return copydf\n","\n","def create_one_hot(df):\n","    copydf = df.copy()\n","    one_hot = {}\n","    \n","    # filter all columns except ID and CLASS\n","    cols = copydf.loc[:, ~df.columns.isin(['ID', 'CLASS'])]\n","    # filter only categorial data\n","    cols = cols.select_dtypes(include=[\"object\", \"category\"])\n","    \n","    for col in cols:\n","        # convert to get categories\n","        if copydf[col].dtype == 'object':\n","            copydf[col] = copydf[col].astype('category')\n","            \n","        # convert categorial to binary (one-hot)    \n","        dummies = pd.get_dummies(copydf[col])\n","\n","        # create binary column for each category\n","        one_hot[col] = {}\n","        for cat in dummies:\n","            one_hot[col][cat] = col + '-' + cat\n","            copydf[col + '-' + cat] = dummies[cat]\n","            # convert to float type (hint 4)\n","            copydf[col + '-' + cat] = copydf[col + '-' + cat].astype('float')\n","            \n","        # drop original columns     \n","        copydf.drop(col, axis=1, inplace=True)\n","    \n","    return copydf, one_hot\n","\n","def apply_one_hot(df, one_hot):\n","    copydf = df.copy()\n","    \n","    for col in one_hot:\n","        # convert categorial to binary (one-hot)\n","        dummies = pd.get_dummies(copydf[col])\n","        # iterate over categories generated in one-hot\n","        for (cat, col_name) in one_hot[col].items():\n","            # for each category take dummy value\n","            copydf[col_name] = dummies[cat]\n","            # convert to float type (hint 4)\n","            copydf[col_name] = copydf[col_name].astype('float')\n","        copydf.drop(col, axis=1, inplace=True)\n","    \n","    return copydf\n","\n","#################################\n","#                               #\n","# Performance measure functions #\n","#                               #\n","#################################\n","\n","def accuracy(df, correctlabels):\n","    return sum(df.idxmax(axis=1)==correctlabels)/len(correctlabels)\n","\n","def brier_score(df, correctlabels):\n","    score = 0\n","    \n","    # get observed probabilities (one for each correct label, otherwise zero)\n","    observed_probs = pd.get_dummies(correctlabels)\n","    # vectorized formula slides brier score (probability - observed_probability) squared\n","    score = (df - observed_probs) ** 2\n","    # sum over different labels, and sum all instances\n","    score = score.sum(axis=1)\n","    # average over instances\n","    score = score.mean()\n","        \n","    return score\n","\n","# function for one label , returns tpr \n","def auc_single(predictions, correctlabels, threshold, c):\n","   \n","    # array with true for correct labels for class c (by row index)\n","    correctlabels_class = np.array(correctlabels)==predictions.columns[c]\n","    \n","    # array with predictions for all instances that should be classified class c\n","    predictions_class = predictions[ predictions.columns[c] ]\n","    \n","    # array with true for all correctly predicted labels according to threshold\n","    predicted_labels = predictions_class[correctlabels_class] >= threshold\n","    pos = sum(predicted_labels)\n","    \n","    # correctly predicted instances (according to threshold) divided by total number of instances that should be class c\n","    tpr = pos / sum(correctlabels_class)\n","    \n","    # repeat for false positive rate (instances not in class)\n","    not_correctlabels_class = np.array(correctlabels)!=predictions.columns[c]\n","    predictions_class = predictions[ predictions.columns[c] ]\n","    predicted_labels = predictions_class[not_correctlabels_class] >= threshold\n","    neg = sum(predicted_labels)\n","    fpr = neg / sum(not_correctlabels_class)\n","    \n","    return tpr, fpr\n","\n","\n","def auc(predictions, correctlabels):\n","    thresholds = np.unique(predictions)\n","    total_number_of_labels = len(correctlabels)\n","    \n","    AUCs = {}\n","    \n","    # iterate over all classes and calculate the area under the ROC(tpr/fpr) curve (AUC)\n","    for (idx,c) in enumerate(np.unique(correctlabels)):\n","        single = [auc_single(predictions, correctlabels, t, idx) for t in reversed(thresholds)]\n","                    \n","        # calculate AUC as area under the curve\n","        AUC = 0\n","        tpr_last = 0\n","        fpr_last = 0\n","        \n","        # iterate over all thresholds\n","        for s in single:\n","            tpr, fpr = s\n","            \n","            # Case 1.) Add area under triangle        \n","            if tpr > tpr_last and fpr > fpr_last:\n","                AUC += (fpr-fpr_last)*tpr_last + (fpr-fpr_last)*(tpr-tpr_last) / 2\n","            \n","            # Case 2.) Add area under rectangle            \n","            elif fpr > fpr_last:\n","                AUC += (fpr-fpr_last)*tpr\n","            \n","            # update point coordinates (tpr, fpr) of curve\n","            tpr_last = tpr\n","            fpr_last = fpr\n","       \n","        AUCs[c] = AUC\n","        \n","                \n","    # take the weighted average for all classes (dependent on their frequency of occourance)\n","    AUC_total = 0\n","    for (cName,auc) in AUCs.items():\n","        number_of_labels = np.sum(np.array(correctlabels) == cName)\n","        weight = number_of_labels / total_number_of_labels\n","        AUC_total += weight * auc\n","        \n","    return AUC_total  \n","\n","def create_bins(df, nobins=10, bintype=\"equal-width\"):\n","    copydf = df.copy()\n","    binning = {}\n","\n","    cols = copydf.loc[:, ~df.columns.isin(['ID', 'CLASS'])]\n","    numbercols = cols.select_dtypes(include=[np.int_, np.float_])\n","\n","    for col in numbercols:\n","        if (bintype == \"equal-width\"):\n","            copydf[col], bins = pd.cut(copydf[col], nobins, labels=False, retbins=True)\n","        else:\n","            copydf[col], bins = pd.qcut(copydf[col], nobins, labels=False, retbins=True, duplicates=\"drop\")\n","        \n","        # hint 4-5\n","        copydf[col] = copydf[col].astype('category', categories=list(range(nobins)))\n","\n","        # hint 6\n","        bins[0] = -np.inf\n","        bins[-1] = np.inf\n","        binning[col] = bins\n","\n","    return copydf, binning\n","\n","\n","def apply_bins(df, binning):\n","    copydf = df.copy()\n","\n","    for column in binning:\n","        copydf[column] = pd.cut(copydf[column], labels=False, bins=binning[column])\n","\n","        # hint 3-4\n","        nobins = len(binning[column] - 1)  # n bins will generate n+1 values\n","        copydf[column] = copydf[column].astype('category', categories=list(range(nobins)))\n","\n","    return copydf\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vsYRr_pk4JQG","colab_type":"text"},"cell_type":"markdown","source":["## 1. Define the class DecisionTree"]},{"metadata":{"colab_type":"text","id":"iBH_ucARsHBy"},"cell_type":"markdown","source":[""]},{"metadata":{"id":"261VG5nV4JQJ","colab_type":"code","outputId":"44676236-bd9c-4daf-f1a9-c7b76da02e48","executionInfo":{"status":"error","timestamp":1543697295468,"user_tz":-60,"elapsed":4832,"user":{"displayName":"Kaikuns","photoUrl":"","userId":"02565776172995128191"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"cell_type":"code","source":["# Define the class DecisionTree with three functions __init__, fit and predict (after the comments):\n","#\n","# Input to __init__: \n","# self: the object itself\n","#\n","# Output from __init__:\n","# nothing\n","# \n","# This function does not return anything but just initializes the following attributes of the object (self) to None:\n","# binning, imputatiom, labels, model\n","#\n","# Input to fit:\n","# self: the object itself\n","# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n","# nobins: no. of bins (default = 10)\n","# bintype: either \"equal-width\" (default) or \"equal-size\"\n","# min_samples_split: no. of instances required to allow a split (default = 5)\n","#\n","# Output from fit:\n","# nothing\n","#\n","# The result of applying this function should be:\n","#\n","# self.binning should be a discretization mapping (see Assignment 1) from df\n","# self.imputation should be an imputation mapping (see Assignment 1) from df\n","# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n","# self.model should be a decision tree (for details, see lecture slides), where the leafs return class probabilities\n","# Note that the function does not return anything but just assigns values to the attributes of the object.\n","#\n","# Hint 1: First find the available features (excluding \"CLASS\" and \"ID\"), then find the class counts, e.g., using \n","#         groupby, and calculate the default class probabilities (relative frequencies of the class labels)\n","# Hint 2: Define a function, e.g., called divide_and_conquer, that takes the above as input together with df \n","#         and min_samples_split, and also a nodeno (starting with 0) to keep track of the generated nodes in the tree\n","# Hint 3: You may represent the tree under construction as a list of nodes (tuples), on the form:\n","#         (nodeno,\"leaf\",class_probabilities): corresponding to a leaf node where class_probabilities is a vector\n","#                                              with the relative class frequencies (ordered according to self.labels)\n","#         (nodeno,feature,node_dict): corresponding to an internal (non-leaf) node where node_dict is a mapping from\n","#                                     the possible values of feature to child nodes (their nodenos)\n","# Hint 4: You may evaluate each feature by a function information_content, which takes the group sizes\n","#         for each possible value of the feature together with the class counts of each group as input\n","# Hint 5: The best feature found (with lowest resulting information content) will be used to split the training\n","#         instances, and each sub-group is used for generating a sub-tree (recursively by divide_and_conquer,\n","#         see lecture slides for details)\n","# Hint 6: The list of nodes output by divide_and_conquer may finally be converted to an array, where each nodeno in the \n","#         tuples corresponds to an index of the array \n","#\n","# Input to predict:\n","# self: the object itself\n","# df: a dataframe\n","# \n","# Output from predict:\n","# predictions: a dataframe with class labels as column names and the rows corresponding to\n","#              predictions with estimated class probabilities for each row in df, where the class probabilities\n","#              are the relative class frequencies in the leaves of the decision tree into which the instances in\n","#              df fall\n","#\n","# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n","# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(nodeno,row), which for a test row\n","#         finds a leaf node from which class probabilities are obtained\n","# Hint 3: This sub-function may recursively traverse the tree (represented by an array), starting with the nodeno\n","#         that corresponds to the root\n","\n","class DecisionTree():\n","    def __init__(self):\n","        self.binning = None\n","        self.imputation = None\n","        self.labels = None\n","        self.model = None\n","        \n","        # Additional to save all possible unique values per feature\n","        self.unique_values = None\n","        \n","    def predict(self, df):\n","        copydf = df.loc[:, ~df.columns.isin(['ID', 'CLASS'])]\n","        copydf_imputation = apply_imputation(copydf, self.imputation)\n","        copydf_bin = apply_bins(copydf_imputation, self.binning)\n","        \n","        \n","        # initialize probabilities\n","        df_probs = pd.DataFrame(columns=self.labels)\n","        \n","        # Make a prediction for each test-row\n","        for idx,row in copydf_bin.iterrows():\n","            row_probs = self.make_prediction(row)\n","            df_probs = df_probs.append(row_probs, ignore_index=True)\n","                        \n","        return df_probs\n","    \n","    def print(self):\n","        pp = pprint.PrettyPrinter(indent=4)\n","        pp.pprint(self.model)\n","        \n","        \n","    def make_prediction(self, row):\n","        # initialize with root\n","        node = self.model\n","        \n","        #print(\"#### Prediction ####\")\n","        #print(row)\n","        \n","        # traverse the tree\n","        while (node[1] != \"leaf\"):\n","            feature = node[1]\n","            feature_val = row[feature]\n","            #print(\"feature\", feature)\n","            #print(\"feature_val\", feature_val)\n","            #print(\"row\", row)\n","            #print(\"node\", node)\n","            #print(\"node_dict\", node[2])\n","            node = node[2][feature_val]\n","                    \n","        # return probabilities of leaf node\n","        #print(\"return\", node[2])\n","        return node[2]\n","        \n","        \n","        \n","    def fit(self, df, nobins=10, bintype=\"equal-width\", min_samples_split=5):\n","        copydf_bins, self.binning = create_bins(df, nobins, bintype)\n","        copydf_imputaton, self.imputation = create_imputation(copydf_bins)\n","        self.labels = pd.unique(copydf_imputaton[\"CLASS\"].astype('category'))  \n","        \n","        # available features\n","        copydf = copydf_imputaton.loc[:, ~df.columns.isin(['ID', 'CLASS'])]\n","        features = copydf.columns\n","        self.unique_values = {f:copydf[f].unique() for idx,f in enumerate(features)}\n","        \n","        # create decision tree\n","        self.model = self.divide_and_conquer(copydf_imputaton, features, self.majority_class(copydf_imputaton), min_samples_split, 0)\n","        \n","        \n","    # input instances, features, majority label, min_split number (when to stop), and initial node number\n","    def divide_and_conquer(self, df, features, label, min_sample_split, nodeno):\n","        # 1.) Exit conditions\n","        probabilities = {v:0.0 for i,v in enumerate(self.labels)}\n","        np.zeros(len(self.labels))    \n","        if (len(df) == 0):\n","            probabilities[label] = 1.0\n","            return (nodeno, \"leaf\", probabilities) # return majority_label (prev call)\n","        if (len(pd.unique(df['CLASS'])) == 1):\n","            probabilities[df['CLASS'].iloc[0]] = 1.0\n","            return (nodeno, \"leaf\", probabilities) # return unique label of all instances\n","        if (len(features) == 0):\n","            probabilities[self.majority_class(df)] = 1.0\n","            return (nodeno, \"leaf\", probabilities) # return majority class (this call)\n","        if (len(df) < min_sample_split):\n","            return (nodeno, \"leaf\", self.probabilities(df)) # return majority class (this call)\n","        \n","        \n","        # 2.) Decide which feature to split with\n","        ent = self.entropy(df['CLASS'].value_counts()) # total entropy for current node\n","        feature_info_gain = np.zeros(len(features))\n","        \n","        # Iterate over features and calculate their information gain\n","        for idx, feature in enumerate(features):\n","            feature_info_gain[idx] = ent - self.residual_information(df, feature)\n","            \n","        # Take feature with minimal bits entropy  \n","        max_idx = np.argmax(feature_info_gain)\n","        best_feature = features[max_idx]\n","        unique_values = self.unique_values[best_feature]\n","        remaining_features = [e for idx,e in enumerate(features) if idx!=max_idx]  # all except taken feature\n","        \n","        #print('')\n","        #print(df)\n","        #print(\"Order features\", features)\n","        #print('Info-Gains:', feature_info_gain)\n","        #print(\"Best feature\", best_feature)\n","        #print(\"min index\", max_idx)\n","        #print(\"features\", features)\n","        #print(\"unique vals\", unique_values)\n","        #print(\"remaining features\", remaining_features)\n","        #print(\" \")\n","\n","        \n","        # 3.) Call divide and conquer recursively for all possible values of the feature (subgroups of instances)\n","        node_dict = {val:self.divide_and_conquer(\n","            df[df[best_feature] == val], # filter only rows where feature has the value \n","            remaining_features,\n","            self.majority_class(df),\n","            min_sample_split,\n","            nodeno + i # TODO: Node number correct increment (trivial)\n","        ) for i,val in enumerate(unique_values)}\n","        node = (nodeno,best_feature,node_dict)\n","        \n","        return node\n","    \n","    def probabilities(self, df):\n","        label_counts = df['CLASS'].value_counts()\n","        probabilities = label_counts / sum(label_counts)\n","        return probabilities\n","    \n","    def majority_class(self, df):\n","        return df['CLASS'].value_counts().argmax()\n","    \n","    # Input: sizes of groups \n","    def residual_information(self, df, feature):\n","        # How often does one feature-value appear [weight]\n","        group_sizes = df[feature].value_counts()\n","        \n","        # The total label counts for the feature (for info-gain)\n","        total_label_counts = sum(df['CLASS'].value_counts())\n","        \n","        infRes = 0\n","        for idx, group_size in group_sizes.iteritems():\n","            label_counts = df[df[feature] == idx]['CLASS'].value_counts()\n","            # relative frequency of feature value * Entropy for the sub-partition\n","            infRes += (group_size / total_label_counts) * self.entropy(label_counts)\n","        return infRes\n","    \n","    def entropy(self, label_counts):\n","        total_no = sum(label_counts)\n","        probabilities = label_counts / total_no\n","        return - sum( probabilities * np.log(probabilities))\n","        \n","\n","\n","# Test your code (leave this part unchanged, except for if auc is undefined)\n","\n","glass_train_df = pd.read_csv(\"glass_train.txt\")\n","\n","glass_test_df = pd.read_csv(\"glass_test.txt\")\n","\n","tree_model = DecisionTree()\n","\n","test_labels = glass_test_df[\"CLASS\"]\n","\n","nobins_values = [5,10]\n","bintype_values = [\"equal-width\",\"equal-size\"]\n","min_samples_split_values = [3,5,10]\n","parameters = [(nobins,bintype,min_samples_split) for nobins in nobins_values for bintype in bintype_values \n","              for min_samples_split in min_samples_split_values]\n","#parameters = [(5, \"equal-width\", 3)]\n","\n","results = np.empty((len(parameters),3))\n","\n","for i in range(len(parameters)):\n","    t0 = time.perf_counter()\n","    tree_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1],min_samples_split=parameters[i][2])\n","    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    t0 = time.perf_counter()\n","    predictions = tree_model.predict(glass_test_df)\n","    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n","                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n","\n","#fast_results = np.array([results[0], ] * 12)\n","\n","#tree_model.print()\n","\n","\n","results = pd.DataFrame(fast_results,index=pd.MultiIndex.from_product([nobins_values,bintype_values,min_samples_split_values]),\n","                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n","\n","results"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:273: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:133: FutureWarning: 'argmax' is deprecated, use 'idxmax' instead. The behavior of 'argmax'\n","will be corrected to return the positional maximum in the future.\n","Use 'series.values.argmax' to get the position of the maximum now.\n"],"name":"stderr"},{"output_type":"stream","text":["Training time (5, 'equal-width', 3): 3.04 s.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:291: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n"],"name":"stderr"},{"output_type":"stream","text":["Testing time (5, 'equal-width', 3): 0.46 s.\n","Training time (5, 'equal-width', 5): 2.71 s.\n","Testing time (5, 'equal-width', 5): 0.45 s.\n","Training time (5, 'equal-width', 10): 1.50 s.\n","Testing time (5, 'equal-width', 10): 0.51 s.\n","Training time (5, 'equal-size', 3): 3.49 s.\n","Testing time (5, 'equal-size', 3): 0.47 s.\n","Training time (5, 'equal-size', 5): 1.85 s.\n","Testing time (5, 'equal-size', 5): 0.49 s.\n","Training time (5, 'equal-size', 10): 0.86 s.\n","Testing time (5, 'equal-size', 10): 0.46 s.\n","Training time (10, 'equal-width', 3): 5.35 s.\n","Testing time (10, 'equal-width', 3): 0.44 s.\n","Training time (10, 'equal-width', 5): 3.35 s.\n","Testing time (10, 'equal-width', 5): 0.44 s.\n","Training time (10, 'equal-width', 10): 1.86 s.\n","Testing time (10, 'equal-width', 10): 0.44 s.\n","Training time (10, 'equal-size', 3): 3.24 s.\n","Testing time (10, 'equal-size', 3): 0.42 s.\n"],"name":"stdout"}]},{"metadata":{"scrolled":true,"id":"GSDfIq-J4JQP","colab_type":"code","outputId":"6782fda6-a03d-4ad4-ce7e-73fabba8b99f","executionInfo":{"status":"error","timestamp":1543561935636,"user_tz":-60,"elapsed":791,"user":{"displayName":"Kaikuns","photoUrl":"","userId":"02565776172995128191"}},"colab":{"base_uri":"https://localhost:8080/","height":336}},"cell_type":"code","source":["# Test your code (leave this part unchanged, except for if auc is undefined)\n","\n","glass_train_df = pd.read_csv(\"glass_train.txt\")\n","\n","glass_test_df = pd.read_csv(\"glass_test.txt\")\n","\n","tree_model = DecisionTree()\n","\n","test_labels = glass_test_df[\"CLASS\"]\n","\n","nobins_values = [5,10]\n","bintype_values = [\"equal-width\",\"equal-size\"]\n","min_samples_split_values = [3,5,10]\n","parameters = [(nobins,bintype,min_samples_split) for nobins in nobins_values for bintype in bintype_values \n","              for min_samples_split in min_samples_split_values]\n","\n","results = np.empty((len(parameters),3))\n","\n","for i in range(len(parameters)):\n","    t0 = time.perf_counter()\n","    tree_model.fit(glass_train_df,nobins=parameters[i][0],bintype=parameters[i][1],min_samples_split=parameters[i][2])\n","    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    t0 = time.perf_counter()\n","    predictions = tree_model.predict(glass_test_df)\n","    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n","                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n","\n","results = pd.DataFrame(results,index=pd.MultiIndex.from_product([nobins_values,bintype_values,min_samples_split_values]),\n","                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n","\n","results\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["['RI', 'Na', 'Mg', 'Al', 'Si', 'K', 'Ca', 'Ba', 'Fe']\n","963\n","107\n","Training time (5, 'equal-width', 3): 0.03 s.\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:273: FutureWarning: specifying 'categories' or 'ordered' in .astype() is deprecated; pass a CategoricalDtype instead\n"],"name":"stderr"},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-e2148952d804>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training time {0}: {1:.2f} s.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglass_test_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Testing time {0}: {1:.2f} s.\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n","\u001b[0;31mAttributeError\u001b[0m: 'DecisionTree' object has no attribute 'predict'"]}]},{"metadata":{"id":"eGtrzfrF4JQY","colab_type":"code","outputId":"0365ffb1-ed31-4494-b837-7dd1019e86b7","colab":{}},"cell_type":"code","source":["train_labels = glass_train_df[\"CLASS\"]\n","tree_model.fit(glass_train_df,min_samples_split=1)\n","predictions = tree_model.predict(glass_train_df)\n","print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n","print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n","print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy on training set: 0.97\n","AUC on training set: 1.00\n","Brier score on training set: 0.03\n"],"name":"stdout"}]},{"metadata":{"id":"wPdYskmY4JQi","colab_type":"text"},"cell_type":"markdown","source":["### Comment on assumptions, things that do not work properly, etc.\n"]},{"metadata":{"id":"rCG8CzCW4JQl","colab_type":"text"},"cell_type":"markdown","source":["## 2. Define the class DecisionForest"]},{"metadata":{"id":"YTPAODIT4JQm","colab_type":"code","colab":{}},"cell_type":"code","source":["# Define the class DecisionForest with three functions __init__, fit and predict (after the comments):\n","#\n","# Input to __init__: \n","# self: the object itself\n","#\n","# Output from __init__:\n","# nothing\n","# \n","# This function does not return anything but just initializes the following attributes of the object (self) to None:\n","# binning, imputatiom, labels, model\n","#\n","# Input to fit:\n","# self: the object itself\n","# df: a dataframe (where the column names \"CLASS\" and \"ID\" have special meaning)\n","# nobins: no. of bins (default = 10)\n","# bintype: either \"equal-width\" (default) or \"equal-size\"\n","# min_samples_split: no. of instances required to allow a split (default = 5)\n","# random_features: no. of features to evaluate at each split (default = 2), 0 means all features (no random sampling)\n","# notrees: no. of trees in the forest (default = 10)\n","#\n","# Output from fit:\n","# nothing\n","#\n","# The result of applying this function should be:\n","#\n","# self.binning should be a discretization mapping (see Assignment 1) from df\n","# self.imputation should be an imputation mapping (see Assignment 1) from df\n","# self.labels should be the categories of the \"CLASS\" column of df, set to be of type \"category\" \n","# self.model should be a random forest (for details, see lecture slides)\n","# Note that the function does not return anything but just assigns values to the attributes of the object.\n","#\n","# Hint 1: Redefine divide_and_conquer to take one additional argument; random_features, and instead of\n","#         evaluating all features choose a random subset, e.g., by np.random.choice (without replacement)\n","# Hint 2: Generate each tree in the forest from a bootstrap replicate of df, e.g., by np.random.choice \n","#         (with replacement) from the index values of df.\n","#\n","# Input to predict:\n","# self: the object itself\n","# df: a dataframe\n","# \n","# Output from predict:\n","# predictions: a dataframe with class labels as column names and the rows corresponding to\n","#              predictions with estimated class probabilities for each row in df, where the class probabilities\n","#              are the mean of all relative class frequencies in the leaves of the forest into which the instances in\n","#              df fall\n","#\n","# Hint 1: Drop any \"CLASS\" and \"ID\" columns first and then apply imputation and binning\n","# Hint 2: Iterate over the rows calling some sub-function, e.g., make_prediction(row), which for a test row\n","#         finds all leaf nodes and calculates the average of their class probabilities\n","\n","\n","class DecisionForest:\n","    def __init__(self):\n","        self.binning = None\n","        self.imputation = None\n","        self.labels = None\n","        self.model = None\n","    \n","    def fit(self, df, nobins, bintype, min_samples_split, random_features, notrees):\n","        df_copy, self.imputation = create_imputation(df)\n","        df_bins, self.binning = create_bins(df_copy, nobins, bintype)\n","        \n","        self.labels = pd.unique(df_bins[\"CLASS\"].astype('category')) \n","        self.model = \n","    \n","    def predict(self, df):\n","        return predictions\n","    \n","    def divide_and_conquer(self, df, features, majority):\n","        if len(df) == 0:\n","            return majority\n","        if len(pd.unique(df['CLASS'])) == 1:\n","            return pd.unique(df['CLASS'])\n","        if len(features) == 0:\n","            return majority\n","        \n","        \n","        \n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"y2_NN_Ca4JQv","colab_type":"code","outputId":"d1ab5bd0-5846-49b2-b4a3-ed8cad2bf4a2","colab":{}},"cell_type":"code","source":["glass_train_df = pd.read_csv(\"glass_train.txt\")\n","\n","glass_test_df = pd.read_csv(\"glass_test.txt\")\n","\n","forest_model = DecisionForest()\n","\n","test_labels = glass_test_df[\"CLASS\"]\n","\n","min_samples_split_values = [1,2,5]\n","random_features_values = [1,2,5]\n","\n","parameters = [(min_samples_split,random_features) for min_samples_split in min_samples_split_values \n","              for random_features in random_features_values]\n","\n","results = np.empty((len(parameters),3))\n","\n","for i in range(len(parameters)):\n","    t0 = time.perf_counter()\n","    forest_model.fit(glass_train_df,min_samples_split=parameters[i][0],random_features=parameters[i][1])\n","    print(\"Training time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    t0 = time.perf_counter()\n","    predictions = forest_model.predict(glass_test_df)\n","    print(\"Testing time {0}: {1:.2f} s.\".format(parameters[i],time.perf_counter()-t0))\n","    results[i] = [accuracy(predictions,test_labels),brier_score(predictions,test_labels),\n","                  auc(predictions,test_labels)] # Assuming that you have defined auc - remove otherwise\n","\n","results = pd.DataFrame(results,index=pd.MultiIndex.from_product([min_samples_split_values,random_features_values]),\n","                       columns=[\"Accuracy\",\"Brier score\",\"AUC\"])\n","\n","results"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Training time (1, 1): 7.51 s.\n","Testing time (1, 1): 0.09 s.\n","Training time (1, 2): 10.65 s.\n","Testing time (1, 2): 0.09 s.\n","Training time (1, 5): 19.78 s.\n","Testing time (1, 5): 0.09 s.\n","Training time (2, 1): 7.97 s.\n","Testing time (2, 1): 0.09 s.\n","Training time (2, 2): 12.01 s.\n","Testing time (2, 2): 0.10 s.\n","Training time (2, 5): 19.42 s.\n","Testing time (2, 5): 0.10 s.\n","Training time (5, 1): 4.57 s.\n","Testing time (5, 1): 0.10 s.\n","Training time (5, 2): 7.31 s.\n","Testing time (5, 2): 0.09 s.\n","Training time (5, 5): 9.75 s.\n","Testing time (5, 5): 0.09 s.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th></th>\n","      <th>Accuracy</th>\n","      <th>Brier score</th>\n","      <th>AUC</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">1</th>\n","      <th>1</th>\n","      <td>0.644860</td>\n","      <td>0.477912</td>\n","      <td>0.860736</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.710280</td>\n","      <td>0.411981</td>\n","      <td>0.892842</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.710280</td>\n","      <td>0.421511</td>\n","      <td>0.895716</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">2</th>\n","      <th>1</th>\n","      <td>0.663551</td>\n","      <td>0.452421</td>\n","      <td>0.872447</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.626168</td>\n","      <td>0.411684</td>\n","      <td>0.911137</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.654206</td>\n","      <td>0.435847</td>\n","      <td>0.881621</td>\n","    </tr>\n","    <tr>\n","      <th rowspan=\"3\" valign=\"top\">5</th>\n","      <th>1</th>\n","      <td>0.644860</td>\n","      <td>0.457859</td>\n","      <td>0.865636</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.719626</td>\n","      <td>0.410616</td>\n","      <td>0.903016</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>0.616822</td>\n","      <td>0.455782</td>\n","      <td>0.883897</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     Accuracy  Brier score       AUC\n","1 1  0.644860     0.477912  0.860736\n","  2  0.710280     0.411981  0.892842\n","  5  0.710280     0.421511  0.895716\n","2 1  0.663551     0.452421  0.872447\n","  2  0.626168     0.411684  0.911137\n","  5  0.654206     0.435847  0.881621\n","5 1  0.644860     0.457859  0.865636\n","  2  0.719626     0.410616  0.903016\n","  5  0.616822     0.455782  0.883897"]},"metadata":{"tags":[]},"execution_count":18}]},{"metadata":{"id":"gy02_U3n4JQ_","colab_type":"code","outputId":"d9154340-eb55-4e34-9d62-9bedc4511beb","colab":{}},"cell_type":"code","source":["train_labels = glass_train_df[\"CLASS\"]\n","forest_model.fit(glass_train_df,min_samples_split=1)\n","predictions = forest_model.predict(glass_train_df)\n","print(\"Accuracy on training set: {0:.2f}\".format(accuracy(predictions,train_labels)))\n","print(\"AUC on training set: {0:.2f}\".format(auc(predictions,train_labels)))\n","print(\"Brier score on training set: {0:.2f}\".format(brier_score(predictions,train_labels)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy on training set: 0.96\n","AUC on training set: 1.00\n","Brier score on training set: 0.12\n"],"name":"stdout"}]},{"metadata":{"id":"3EbLDA8I4JRI","colab_type":"text"},"cell_type":"markdown","source":["### Comment on assumptions, things that do not work properly, etc."]}]}